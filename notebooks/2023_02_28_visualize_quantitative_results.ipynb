{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch_tensorrt  # keep this\n",
    "import piq\n",
    "from tqdm import tqdm\n",
    "from DISTS_pytorch import DISTS\n",
    "\n",
    "from binarization.config import get_default_config\n",
    "from binarization.dataset import get_test_batches\n",
    "from binarization.traintools import (\n",
    "    prepare_generator, prepare_cuda_device, CustomLPIPS\n",
    ")\n",
    "from binarization.datatools import (\n",
    "    min_max_scaler,\n",
    "    inv_make_4times_downscalable,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [✅] load a model\n",
    "- [✅] fetch a pair compressed/original image\n",
    "- [✅] generated = model(compressed)\n",
    "- [✅] metric(original, generated)\n",
    "- [✅] scale up\n",
    "- [] adjust names (e.g., ms_ssim -> MS-SSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(\n",
    "    n_evaluations: int = 50,\n",
    ") -> dict[str, float]:\n",
    "    default_cfg = get_default_config()\n",
    "    device = prepare_cuda_device()\n",
    "\n",
    "    lpips_alex = CustomLPIPS(net=\"alex\")\n",
    "    dists = DISTS()\n",
    "    metrics = defaultdict(list)\n",
    "    model_names = (\"unet\", \"srunet\")\n",
    "\n",
    "    for model_name in model_names:\n",
    "        cfg = default_cfg.copy()\n",
    "        cfg.model.name = model_name\n",
    "        cfg.model.ckpt_path_to_resume = Path(\n",
    "            cfg.paths.artifacts_dir,\n",
    "            \"best_checkpoints\",\n",
    "            f\"2023_03_24_{model_name}_2_191268.pth\",\n",
    "        )\n",
    "        gen = prepare_generator(cfg, device).eval()\n",
    "\n",
    "        test_batches = get_test_batches()\n",
    "        progress_bar = tqdm(test_batches, total=n_evaluations)\n",
    "\n",
    "        for step_id, (original, compressed) in enumerate(progress_bar):\n",
    "            if n_evaluations and step_id > n_evaluations - 1:\n",
    "                break\n",
    "            original = min_max_scaler(original)\n",
    "            compressed = compressed.to(device)\n",
    "\n",
    "            gen.eval()\n",
    "            with torch.no_grad():\n",
    "                generated = gen(compressed).cpu()\n",
    "\n",
    "            width_original = original.shape[-1]\n",
    "            height_original = original.shape[-2]\n",
    "            generated = inv_make_4times_downscalable(\n",
    "                generated,\n",
    "                width_original=width_original,\n",
    "                height_original=height_original,\n",
    "            )\n",
    "\n",
    "            metrics[f\"{model_name}_lpips\"].append(lpips_alex(generated, original).item())\n",
    "            metrics[f\"{model_name}_dists\"].append(dists(generated, original).item())\n",
    "            metrics[f\"{model_name}_brisque\"].append(piq.brisque(generated).item())\n",
    "            metrics[f\"{model_name}_ssim\"].append(piq.ssim(generated, original).item())\n",
    "            metrics[f\"{model_name}_ms_ssim\"].append(piq.multi_scale_ssim(generated, original).item())\n",
    "            metrics[f\"{model_name}_psnr\"].append(piq.psnr(generated, original).item())\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_trt_models(\n",
    "    n_evaluations: int = 50,\n",
    ") -> dict[str, float]:\n",
    "    cfg = get_default_config()\n",
    "    device = prepare_cuda_device()\n",
    "    lpips_alex = CustomLPIPS(net=\"alex\")\n",
    "    dists = DISTS()\n",
    "    metrics = defaultdict(list)\n",
    "\n",
    "    model_names = (\"unet\", \"srunet\")\n",
    "    available_dtypes = (\"fp32\", \"fp16\", \"int8\")\n",
    "\n",
    "    for model_name in model_names:\n",
    "        for dtype in available_dtypes:\n",
    "            quant_gen = torch.jit.load(\n",
    "                cfg.paths.trt_dir / f\"{model_name}_{dtype}.ts\"\n",
    "            ).to(device).eval()\n",
    "\n",
    "            test_batches = get_test_batches()\n",
    "            progress_bar = tqdm(test_batches, total=n_evaluations)\n",
    "\n",
    "            for step_id, (original, compressed) in enumerate(progress_bar):\n",
    "                if n_evaluations and step_id > n_evaluations - 1:\n",
    "                    break\n",
    "                original = min_max_scaler(original)\n",
    "                compressed = compressed.to(device)\n",
    "\n",
    "                if dtype == \"fp16\":\n",
    "                    compressed = compressed.half()\n",
    "                elif dtype not in {\"fp32\", \"int8\"}:\n",
    "                    raise ValueError(\n",
    "                        f\"Unknown dtype: {dtype}. Choose in {'fp32', 'fp16', 'int8'}.\"\n",
    "                    )\n",
    "\n",
    "                quant_gen.eval()\n",
    "                with torch.no_grad():\n",
    "                    generated = quant_gen(compressed).cpu()\n",
    "                width_original = original.shape[-1]\n",
    "                height_original = original.shape[-2]\n",
    "                generated = inv_make_4times_downscalable(\n",
    "                    generated,\n",
    "                    width_original=width_original,\n",
    "                    height_original=height_original,\n",
    "                )\n",
    "\n",
    "                metrics[f\"{model_name}_{dtype}_lpips\"].append(lpips_alex(generated, original).item())\n",
    "                metrics[f\"{model_name}_{dtype}_dists\"].append(dists(generated, original).item())\n",
    "                metrics[f\"{model_name}_{dtype}_brisque\"].append(piq.brisque(generated).item())\n",
    "                metrics[f\"{model_name}_{dtype}_ssim\"].append(piq.ssim(generated, original).item())\n",
    "                metrics[f\"{model_name}_{dtype}_ms_ssim\"].append(piq.multi_scale_ssim(generated, original).item())\n",
    "                metrics[f\"{model_name}_{dtype}_psnr\"].append(piq.psnr(generated, original).item())\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "def save_json(json_obj: dict, save_path: Path):\n",
    "    with open(save_path, \"w\") as out_file:\n",
    "        json.dump(json_obj, out_file)\n",
    "\n",
    "n_evaluations = 60\n",
    "metrics = eval_models(n_evaluations=n_evaluations)\n",
    "trt_metrics = eval_trt_models(n_evaluations=n_evaluations)\n",
    "\n",
    "metrics.update(trt_metrics)\n",
    "today_str = datetime.now().strftime(r\"%Y_%m_%d\")\n",
    "cfg = get_default_config()\n",
    "save_path = cfg.paths.outputs_dir / f\"{today_str}_metrics.json\"\n",
    "save_json(metrics, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# df = pd.DataFrame(metrics)\n",
    "# today_str = datetime.now().strftime(r\"%Y_%m_%d\")\n",
    "# save_path = default_cfg.paths.outputs_dir / f\"{today_str}_results.csv\"\n",
    "# df.to_csv(save_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "today_str = datetime.now().strftime(r\"%Y_%m_%d\")\n",
    "cfg = get_default_config()\n",
    "# metrics_json_path = cfg.paths.outputs_dir / f\"{today_str}_metrics.json\"\n",
    "metrics_json_path = cfg.paths.outputs_dir / f\"2023_03_27_metrics.json\"\n",
    "with open(metrics_json_path, \"r\") as in_file:\n",
    "    metrics_json = json.load(in_file)\n",
    "\n",
    "df = pd.DataFrame(metrics_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = [\n",
    "#     \"unet\",\n",
    "#     \"unet_fp32\",\n",
    "#     \"unet_fp16\",\n",
    "#     \"unet_int8\",\n",
    "#     \"srunet\",\n",
    "#     \"srunet_fp32\",\n",
    "#     \"srunet_fp16\",\n",
    "#     \"srunet_int8\"\n",
    "# ]\n",
    "\n",
    "# info_metrics = {}\n",
    "# metrics_table = {}\n",
    "# for metric in metrics:\n",
    "#     info_metrics[metric] = df[[model + '_' + metric for model in model_names]]\n",
    "#     info_metrics[metric].columns = model_names\n",
    "#     metrics_table[metric] = info_metrics[metric].apply(lambda x: f\"{x.mean().round(3)} ± {x.std().round(3)}\", axis=0)\n",
    "# print(pd.DataFrame(metrics_table).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"unet\",\n",
    "    \"unet_fp32\",\n",
    "    \"unet_fp16\",\n",
    "    \"unet_int8\",\n",
    "    \"srunet\",\n",
    "    \"srunet_fp32\",\n",
    "    \"srunet_fp16\",\n",
    "    \"srunet_int8\"\n",
    "]\n",
    "corrected_model_names = [\n",
    "    \"UNet\", \"UNet-FP32\", \"UNet-FP16\", \"UNet-INT8\",\n",
    "    \"SRUNet\", \"SRUNet-FP32\", \"SRUNet-FP16\", \"SRUNet-INT8\"\n",
    "]\n",
    "model_names_map = {\n",
    "    'unet': 'UNet',\n",
    "    'unet_fp32': 'UNet-FP32',\n",
    "    'unet_fp16': 'UNet-FP16',\n",
    "    'unet_int8': 'UNet-INT8',\n",
    "    'srunet': 'SRUNet',\n",
    "    'srunet_fp32': 'SRUNet-FP32',\n",
    "    'srunet_fp16': 'SRUNet-FP16',\n",
    "    'srunet_int8': 'SRUNet-INT8'\n",
    "}\n",
    "\n",
    "perceptual_metrics = [\"lpips\", \"dists\", \"brisque\"]\n",
    "traditional_metrics = [\"ssim\", \"ms_ssim\", \"psnr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_dir = Path.home() / \"Projects/master-thesis/thesis/static\"\n",
    "perceptual_metrics_savepath = static_dir / \"boxplots_perceptual_metrics.jpg\"\n",
    "traditional_metrics_savepath = static_dir / \"boxplots_traditional_metrics.jpg\"\n",
    "\n",
    "dfs = {}\n",
    "for metric in perceptual_metrics:\n",
    "    dfs[metric] = df[[model + \"_\" + metric for model in models]]\n",
    "    dfs[metric].columns = corrected_model_names\n",
    "fig, ax = plt.subplots(1, len(perceptual_metrics), figsize=(16, 4), sharey=True)\n",
    "for i, k in enumerate(perceptual_metrics):\n",
    "    sns.boxplot(dfs[k], ax=ax[i], orient=\"h\")\n",
    "    ax[i].set_title(k.upper(), fontsize=16)\n",
    "    ax[i].grid()\n",
    "fig.savefig(perceptual_metrics_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for metric in traditional_metrics:\n",
    "    dfs[metric] = df[[model + \"_\" + metric for model in models]]\n",
    "    dfs[metric].columns = corrected_model_names\n",
    "fig, ax = plt.subplots(1, len(traditional_metrics), figsize=(16, 4), sharey=True)\n",
    "for i, k in enumerate(traditional_metrics):\n",
    "    sns.boxplot(dfs[k], ax=ax[i], orient=\"h\")\n",
    "    ax[i].set_title(k.replace(\"_\", \"-\").upper(), fontsize=16)\n",
    "    ax[i].grid()\n",
    "fig.savefig(traditional_metrics_savepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "cfg = get_default_config()\n",
    "today_str = datetime.now().strftime(r\"%Y_%m_%d\")\n",
    "# unet_timings_json_path = cfg.paths.outputs_dir / f\"{today_str}_timings_unet.json\"\n",
    "# srunet_timings_json_path = cfg.paths.outputs_dir / f\"{today_str}_timings_srunet.json\"\n",
    "unet_timings_json_path = cfg.paths.outputs_dir / f\"2023_03_30_timings_unet.json\"\n",
    "srunet_timings_json_path = cfg.paths.outputs_dir / f\"2023_03_30_timings_srunet.json\"\n",
    "\n",
    "with open(unet_timings_json_path, \"r\") as in_file:\n",
    "    unet_timings_dict = json.load(in_file)\n",
    "with open(srunet_timings_json_path, \"r\") as in_file:\n",
    "    srunet_timings_dict = json.load(in_file)\n",
    "unet_timings_dict.update(srunet_timings_dict)\n",
    "timings = pd.DataFrame(unet_timings_dict)\n",
    "timings /= 1e+9\n",
    "timings.rename(columns=model_names_map, inplace=True)\n",
    "# save_path = cfg.paths.outputs_dir / f\"{today_str}_timings.csv\"\n",
    "# timings.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings_table = timings.apply(lambda x: f\"{x.mean().round(5):.5f} ± {x.std().round(5):.5f}\", axis=0).to_frame(name=\"times [s]\")\n",
    "print(timings_table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptual_metrics_info = {}\n",
    "perceptual_metrics_table = {}\n",
    "for metric in perceptual_metrics:\n",
    "    perceptual_metrics_info[metric] = df[[model + '_' + metric for model in models]]\n",
    "    perceptual_metrics_info[metric].columns = corrected_model_names\n",
    "    perceptual_metrics_table[metric.replace(\"_\", \"-\").upper()] = perceptual_metrics_info[metric].apply(lambda x: f\"{x.mean().round(5):.5f} ± {x.std().round(5):.5f}\", axis=0)\n",
    "print(pd.DataFrame(perceptual_metrics_table).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traditional_metrics_info = {}\n",
    "traditional_metrics_table = {}\n",
    "for metric in traditional_metrics:\n",
    "    traditional_metrics_info[metric] = df[[model + '_' + metric for model in models]]\n",
    "    traditional_metrics_info[metric].columns = corrected_model_names\n",
    "    traditional_metrics_table[metric.replace(\"_\", \"-\").upper()] = traditional_metrics_info[metric].apply(lambda x: f\"{x.mean().round(5):.5f} ± {x.std().round(5):.5f}\", axis=0)\n",
    "print(pd.DataFrame(traditional_metrics_table).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4), sharex=True)\n",
    "sns.boxplot(timings, ax=ax, orient=\"h\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"seconds\")\n",
    "fig.suptitle(\"Time elapsed generating one image\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(static_dir / \"boxplots_timings.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vmaf_df.T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmaf_json_path = Path(cfg.paths.artifacts_dir, \"vmaf\", \"vmaf_res.json\")\n",
    "with open(vmaf_json_path, \"r\") as in_file:\n",
    "    vmaf_json = json.load(in_file)\n",
    "\n",
    "vmaf_df = pd.DataFrame(vmaf_json)\n",
    "# vmaf_df.columns = [x + \"_vmaf\" for x in vmaf_df.columns]\n",
    "vmaf_df.index = (\"mean\", \"harmonic_mean\")\n",
    "\n",
    "vmaf_means = vmaf_df.loc[\"mean\"]\n",
    "vmaf_xlim = vmaf_means.min(), vmaf_means.max()\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "vmaf_df.loc[\"mean\"][[x for x in vmaf_df.columns if x.startswith(\"unet\")]].plot(kind=\"barh\", ax=ax1)\n",
    "vmaf_df.loc[\"mean\"][[x for x in vmaf_df.columns if x.startswith(\"srunet\")]].plot(kind=\"barh\", ax=ax2)\n",
    "ax2.set_xlabel(\"VMAF (mean)\")\n",
    "ax1.grid(\"on\"); ax2.grid(\"on\")\n",
    "\n",
    "vmaf_means = vmaf_df.loc[\"harmonic_mean\"]\n",
    "vmaf_xlim = vmaf_means.min(), vmaf_means.max()\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "vmaf_df.loc[\"harmonic_mean\"][[x for x in vmaf_df.columns if x.startswith(\"unet\")]].plot(kind=\"barh\", ax=ax1)\n",
    "vmaf_df.loc[\"harmonic_mean\"][[x for x in vmaf_df.columns if x.startswith(\"srunet\")]].plot(kind=\"barh\", ax=ax2)\n",
    "ax2.set_xlabel(\"VMAF (harmonic mean)\")\n",
    "ax1.grid(\"on\"); ax2.grid(\"on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f774474b43a20ce26305101b7f5844f986dc13ec8a91c410b215dc004257e9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
