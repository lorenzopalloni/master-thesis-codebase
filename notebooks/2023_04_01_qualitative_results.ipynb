{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script to evaluate an image with a super-resolution model\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_tensorrt  # mandatory for inference even without calling it\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from binarization.config import Gifnoc, get_default_config\n",
    "from binarization.dataset import get_test_batches\n",
    "from binarization.traintools import prepare_cuda_device, prepare_generator\n",
    "\n",
    "from binarization.datatools import (\n",
    "    compose,\n",
    "    list_directories,\n",
    "    list_files,\n",
    "    make_4times_downscalable,\n",
    "    min_max_scaler,\n",
    "    random_crop_images,\n",
    "    draw_validation_fig,\n",
    "    postprocess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_default_config()\n",
    "cuda_or_cpu = prepare_cuda_device(0)\n",
    "\n",
    "generated_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = cfg.paths.original_frames_dir / \"old_town_cross_1080p50/old_town_cross_1080p50_0001.png\"\n",
    "compressed_path = cfg.paths.compressed_frames_dir / \"old_town_cross_1080p50/old_town_cross_1080p50_0001.jpg\"\n",
    "original, compressed = Image.open(original_path), Image.open(compressed_path)\n",
    "pipe = compose(TF.pil_to_tensor, min_max_scaler, make_4times_downscalable)\n",
    "original, compressed = TF.pil_to_tensor(original), TF.pil_to_tensor(compressed)\n",
    "compressed = min_max_scaler(compressed)\n",
    "compressed = make_4times_downscalable(compressed)\n",
    "compressed = compressed.to(cuda_or_cpu)\n",
    "half_compressed = compressed.half()\n",
    "\n",
    "unet_fp32_path = cfg.paths.trt_dir / \"unet_fp32.ts\"\n",
    "unet_fp32 = torch.jit.load(unet_fp32_path).to(cuda_or_cpu).eval()\n",
    "with torch.no_grad():\n",
    "    generated_dict[\"UNet-FP32\"] = TF.crop(\n",
    "        postprocess(\n",
    "            unet_fp32(compressed.unsqueeze(0)).squeeze(0).cpu(),\n",
    "            width_original=original.shape[-1],\n",
    "            height_original=original.shape[-2],\n",
    "        ),\n",
    "        150 * 4, 250 * 4, 96 * 4, 96 * 4\n",
    "    )\n",
    "unet_fp16_path = cfg.paths.trt_dir / \"unet_fp16.ts\"\n",
    "unet_fp16 = torch.jit.load(unet_fp16_path).to(cuda_or_cpu).eval()\n",
    "with torch.no_grad():\n",
    "    generated_dict[\"UNet-FP16\"] = TF.crop(\n",
    "        postprocess(\n",
    "            unet_fp16(half_compressed.unsqueeze(0)).squeeze(0).cpu(),\n",
    "            width_original=original.shape[-1],\n",
    "            height_original=original.shape[-2],\n",
    "        ),\n",
    "        150 * 4, 250 * 4, 96 * 4, 96 * 4\n",
    "    )\n",
    "unet_int8_path = cfg.paths.trt_dir / \"unet_int8.ts\"\n",
    "unet_int8 = torch.jit.load(unet_int8_path).to(cuda_or_cpu).eval()\n",
    "with torch.no_grad():\n",
    "    generated_dict[\"UNet-INT8\"] = TF.crop(\n",
    "        postprocess(\n",
    "            unet_int8(compressed.unsqueeze(0)).squeeze(0).cpu(),\n",
    "            width_original=original.shape[-1],\n",
    "            height_original=original.shape[-2],\n",
    "        ),\n",
    "        150 * 4, 250 * 4, 96 * 4, 96 * 4\n",
    "    )\n",
    "\n",
    "srunet_fp32_path = cfg.paths.trt_dir / \"srunet_fp32.ts\"\n",
    "srunet_fp32 = torch.jit.load(srunet_fp32_path).to(cuda_or_cpu).eval()\n",
    "with torch.no_grad():\n",
    "    generated_dict[\"SRUNet-FP32\"] = TF.crop(\n",
    "        postprocess(\n",
    "            srunet_fp32(compressed.unsqueeze(0)).squeeze(0).cpu(),\n",
    "            width_original=original.shape[-1],\n",
    "            height_original=original.shape[-2],\n",
    "        ),\n",
    "        150 * 4, 250 * 4, 96 * 4, 96 * 4\n",
    "    )\n",
    "srunet_fp16_path = cfg.paths.trt_dir / \"srunet_fp16.ts\"\n",
    "srunet_fp16 = torch.jit.load(srunet_fp16_path).to(cuda_or_cpu).eval()\n",
    "with torch.no_grad():\n",
    "    generated_dict[\"SRUNet-FP16\"] = TF.crop(\n",
    "        postprocess(\n",
    "            srunet_fp16(half_compressed.unsqueeze(0)).squeeze(0).cpu(),\n",
    "            width_original=original.shape[-1],\n",
    "            height_original=original.shape[-2],\n",
    "        ),\n",
    "        150 * 4, 250 * 4, 96 * 4, 96 * 4\n",
    "    )\n",
    "srunet_int8_path = cfg.paths.trt_dir / \"srunet_int8.ts\"\n",
    "srunet_int8 = torch.jit.load(srunet_int8_path).to(cuda_or_cpu).eval()\n",
    "with torch.no_grad():\n",
    "    generated_dict[\"SRUNet-INT8\"] = TF.crop(\n",
    "        postprocess(\n",
    "            srunet_int8(compressed.unsqueeze(0)).squeeze(0).cpu(),\n",
    "            width_original=original.shape[-1],\n",
    "            height_original=original.shape[-2],\n",
    "        ),\n",
    "        150 * 4, 250 * 4, 96 * 4, 96 * 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = cfg.paths.original_frames_dir / \"old_town_cross_1080p50/old_town_cross_1080p50_0001.png\"\n",
    "compressed_path = cfg.paths.compressed_frames_dir / \"old_town_cross_1080p50/old_town_cross_1080p50_0001.jpg\"\n",
    "original, compressed = Image.open(original_path), Image.open(compressed_path)\n",
    "pipe = compose(TF.pil_to_tensor, min_max_scaler, make_4times_downscalable)\n",
    "original, compressed = TF.pil_to_tensor(original), TF.pil_to_tensor(compressed)\n",
    "\n",
    "compressed = TF.crop(compressed, 150, 250, 96, 96)\n",
    "original = TF.crop(original, 150 * 4, 250 * 4, 96 * 4, 96 * 4)\n",
    "compressed = min_max_scaler(compressed)\n",
    "\n",
    "compressed = compressed.to(cuda_or_cpu)\n",
    "\n",
    "unet_ckpt_path = Path(\n",
    "    cfg.paths.artifacts_dir,\n",
    "    \"best_checkpoints\",\n",
    "    f\"2023_03_24_unet_2_191268.pth\",\n",
    ")\n",
    "unet_cfg = cfg.copy()\n",
    "unet_cfg.model.ckpt_path_to_resume = unet_ckpt_path\n",
    "unet_cfg.model.name = \"unet\"\n",
    "\n",
    "unet = prepare_generator(unet_cfg, device=cuda_or_cpu).eval()\n",
    "with torch.no_grad():\n",
    "    generated_dict[\"UNet\"] = postprocess(\n",
    "        unet(compressed.unsqueeze(0)).squeeze(0).cpu(),\n",
    "        width_original=original.shape[-1],\n",
    "        height_original=original.shape[-2],\n",
    "    )\n",
    "\n",
    "srunet_ckpt_path = Path(\n",
    "    cfg.paths.artifacts_dir,\n",
    "    \"best_checkpoints\",\n",
    "    f\"2023_03_24_srunet_2_191268.pth\",\n",
    ")\n",
    "srunet_cfg = cfg.copy()\n",
    "srunet_cfg.model.ckpt_path_to_resume = srunet_ckpt_path\n",
    "srunet_cfg.model.name = \"srunet\"\n",
    "\n",
    "srunet = prepare_generator(srunet_cfg, device=cuda_or_cpu).eval()\n",
    "with torch.no_grad():\n",
    "    generated_dict[\"SRUNet\"] = postprocess(\n",
    "        srunet(compressed.unsqueeze(0)).squeeze(0).cpu(),\n",
    "        width_original=original.shape[-1],\n",
    "        height_original=original.shape[-2],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = compressed.cpu()\n",
    "figsize=(10, 3 * 4)\n",
    "original_image_pil = TF.to_pil_image(original)\n",
    "compressed_image_pil = TF.to_pil_image(compressed)\n",
    "fig, ax = plt.subplots(4, 3, figsize=figsize)\n",
    "\n",
    "for i, model_name in enumerate([\"UNet\", \"UNet-FP32\", \"UNet-FP16\", \"UNet-INT8\"]):\n",
    "    ax[i][0].imshow(original_image_pil)\n",
    "    ax[i][0].set_title('high quality')\n",
    "    ax[i][0].axis('off')\n",
    "    ax[i][1].imshow(TF.to_pil_image(generated_dict[model_name]))\n",
    "    ax[i][1].set_title(f'generated by {model_name}')\n",
    "    ax[i][1].axis('off')\n",
    "    ax[i][2].imshow(compressed_image_pil)\n",
    "    ax[i][2].set_title('low quality')\n",
    "    ax[i][2].axis('off')\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    top=1.0, bottom=0.0, right=1.0, left=0.0, hspace=0.15, wspace=0.0\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.savefig(cfg.paths.outputs_dir / \"unet_qualitative_results.png\")\n",
    "# fig.savefig(save_dir / f'{step_id:05d}_validation_fig.png')\n",
    "# plt.close(fig)  # close the current fig to prevent OOM issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = compressed.cpu()\n",
    "figsize=(10, 3 * 4)\n",
    "original_image_pil = TF.to_pil_image(original)\n",
    "compressed_image_pil = TF.to_pil_image(compressed)\n",
    "fig, ax = plt.subplots(4, 3, figsize=figsize)\n",
    "\n",
    "for i, model_name in enumerate([\"SRUNet\", \"SRUNet-FP32\", \"SRUNet-FP16\", \"SRUNet-INT8\"]):\n",
    "    ax[i][0].imshow(original_image_pil)\n",
    "    ax[i][0].set_title('high quality')\n",
    "    ax[i][0].axis('off')\n",
    "    ax[i][1].imshow(TF.to_pil_image(generated_dict[model_name]))\n",
    "    ax[i][1].set_title(f'generated by {model_name}')\n",
    "    ax[i][1].axis('off')\n",
    "    ax[i][2].imshow(compressed_image_pil)\n",
    "    ax[i][2].set_title('low quality')\n",
    "    ax[i][2].axis('off')\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    top=1.0, bottom=0.0, right=1.0, left=0.0, hspace=0.15, wspace=0.0\n",
    ")\n",
    "fig.savefig(cfg.paths.outputs_dir / \"srunet_qualitative_results.png\")\n",
    "# fig.savefig(save_dir / f'{step_id:05d}_validation_fig.png')\n",
    "# plt.close(fig)  # close the current fig to prevent OOM issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_evaluations = 1\n",
    "model_name = \"unet\"\n",
    "cuda_or_cpu = \"cuda\"\n",
    "dtype = \"fp32\"\n",
    "if cuda_or_cpu.startswith(\"cuda\"):\n",
    "    cuda_or_cpu = prepare_cuda_device(0)\n",
    "cfg = get_default_config()\n",
    "\n",
    "ckpt_path = Path(\n",
    "    cfg.paths.artifacts_dir,\n",
    "    \"best_checkpoints\",\n",
    "    f\"2023_03_24_{model_name}_2_191268.pth\",\n",
    ")\n",
    "cfg.model.ckpt_path_to_resume = ckpt_path\n",
    "cfg.model.name = model_name\n",
    "\n",
    "save_dir = cfg.paths.outputs_dir / ckpt_path.stem\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "gen = prepare_generator(cfg, device=cuda_or_cpu).eval()\n",
    "\n",
    "test_batches = get_test_batches(cfg)\n",
    "progress_bar = tqdm(test_batches, total=n_evaluations)\n",
    "for step_id, (original, compressed) in enumerate(progress_bar):\n",
    "    if n_evaluations and step_id > n_evaluations - 1:\n",
    "        break\n",
    "\n",
    "    compressed = compressed.to(cuda_or_cpu)\n",
    "    if dtype == 'fp16':\n",
    "        compressed = compressed.half()\n",
    "    elif dtype not in {'fp32', 'int8'}:\n",
    "        raise ValueError(\n",
    "            f\"Unknown dtype: {dtype}. Choose in {'fp32', 'fp16', 'int8'}.\"\n",
    "        )\n",
    "\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = gen(compressed)\n",
    "\n",
    "    compressed = compressed.cpu()\n",
    "    generated = generated.cpu()\n",
    "    generated = postprocess(\n",
    "        generated=generated,\n",
    "        width_original=original.shape[-1],\n",
    "        height_original=original.shape[-2],\n",
    "    )\n",
    "\n",
    "    for i in range(original.shape[0]):\n",
    "        original_image=original[i]\n",
    "        compressed_image=compressed[i]\n",
    "        generated_image=generated[i]\n",
    "\n",
    "        # compressed_h, compressed_w = \n",
    "        offset = compressed_image.shape[-2] - original_image.shape[-2] // 4\n",
    "        compressed_image = TF.crop(compressed_image, offset // 2, 0, compressed_image.shape[-2] - offset, compressed_image.shape[-1])\n",
    "\n",
    "        figsize=(15, 5 * 6)\n",
    "        original_image_pil = TF.to_pil_image(original_image)\n",
    "        compressed_image_pil = TF.to_pil_image(compressed_image)\n",
    "        generated_image_pil = TF.to_pil_image(generated_image)\n",
    "        fig, ax = plt.subplots(6, 3, figsize=figsize)\n",
    "        ax[0][0].imshow(original_image_pil)\n",
    "        ax[0][0].set_title('high quality')\n",
    "        ax[0][0].axis('off')\n",
    "        ax[0][1].imshow(generated_image_pil)\n",
    "        ax[0][1].set_title(f'generated by {model_name}-{dtype}')\n",
    "        ax[0][1].axis('off')\n",
    "        ax[0][2].imshow(compressed_image_pil)\n",
    "        ax[0][2].set_title('low quality')\n",
    "        ax[0][2].axis('off')\n",
    "        fig.subplots_adjust(\n",
    "            top=1.0, bottom=0.0, right=1.0, left=0.0, hspace=0.0, wspace=0.0\n",
    "        )\n",
    "        # fig.savefig(save_dir / f'{step_id:05d}_validation_fig.jpg')\n",
    "        # plt.close(fig)  # close the current fig to prevent OOM issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_images(\n",
    "    gen: torch.nn.Module,\n",
    "    save_dir: Path,\n",
    "    cfg: Gifnoc = None,\n",
    "    n_evaluations: int | None = None,\n",
    "    dtype: str = \"fp32\",\n",
    "    cuda_or_cpu: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"Upscales a bunch of images given a super-resolution model.\n",
    "\n",
    "    Args:\n",
    "        gen (torch.nn.Module): a PyTorch generator model.\n",
    "        save_dir (Path): path to directory where to save evaluation figures.\n",
    "        cfg (Gifnoc, optional): configuration settings. The only useful\n",
    "            option to be modified here is `cfg.params.buffer_size`.\n",
    "            Defaults to None.\n",
    "        n_evaluations (Union[int, None], optional): num of images to evaluate.\n",
    "            Defaults to None (that means all the available frames).\n",
    "        dtype (str): Choose in {\"fp32\", \"fp16\", \"int8\"}. Defaults to \"fp32\".\n",
    "        cuda_or_cpu (str, optional): {\"cuda\", \"cpu\"}. Defaults to \"cuda\".\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = get_default_config()\n",
    "    if cuda_or_cpu.startswith(\"cuda\"):\n",
    "        cuda_or_cpu = prepare_cuda_device(0)\n",
    "\n",
    "    test_batches = get_test_batches(cfg)\n",
    "    progress_bar = tqdm(test_batches, total=n_evaluations)\n",
    "\n",
    "    for step_id, (original, compressed) in enumerate(progress_bar):\n",
    "        if n_evaluations and step_id > n_evaluations - 1:\n",
    "            break\n",
    "\n",
    "        compressed = compressed.to(cuda_or_cpu)\n",
    "        if dtype == 'fp16':\n",
    "            compressed = compressed.half()\n",
    "        elif dtype not in {'fp32', 'int8'}:\n",
    "            raise ValueError(\n",
    "                f\"Unknown dtype: {dtype}. Choose in {'fp32', 'fp16', 'int8'}.\"\n",
    "            )\n",
    "\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            generated = gen(compressed)\n",
    "\n",
    "        compressed = compressed.cpu()\n",
    "        generated = generated.cpu()\n",
    "        generated = postprocess(\n",
    "            generated=generated,\n",
    "            width_original=original.shape[-1],\n",
    "            height_original=original.shape[-2],\n",
    "        )\n",
    "\n",
    "        for i in range(original.shape[0]):\n",
    "            fig = draw_validation_fig(\n",
    "                original_image=original[i],\n",
    "                compressed_image=compressed[i],\n",
    "                generated_image=generated[i],\n",
    "                figsize=(36, 15),\n",
    "            )\n",
    "            fig.savefig(save_dir / f'{step_id:05d}_validation_fig.jpg')\n",
    "            plt.close(fig)  # close the current fig to prevent OOM issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_evaluations = 1\n",
    "model_name = \"unet\"\n",
    "cuda_or_cpu = \"cuda:0\"\n",
    "cfg = get_default_config()\n",
    "\n",
    "available_dtypes = (\"fp32\", \"fp16\", \"int8\")\n",
    "for dtype in available_dtypes:\n",
    "    quant_save_dir = cfg.paths.outputs_dir / f\"{model_name}_{dtype}\"\n",
    "    quant_save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    quant_path = cfg.paths.trt_dir / f\"{model_name}_{dtype}.ts\"\n",
    "    quant_gen = torch.jit.load(quant_path).to(cuda_or_cpu).eval()\n",
    "\n",
    "    eval_images(\n",
    "        gen=quant_gen,\n",
    "        save_dir=quant_save_dir,\n",
    "        n_evaluations=n_evaluations,\n",
    "        dtype=dtype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_validation_fig(\n",
    "    original_image: torch.Tensor,\n",
    "    compressed_image: torch.Tensor,\n",
    "    generated_image: torch.Tensor,\n",
    "    figsize: tuple[int, int] = (36, 15),\n",
    "    save_path: Path | None = None,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Draws three images in a row with matplotlib.\"\"\"\n",
    "    original_image_pil = TF.to_pil_image(original_image)\n",
    "    compressed_image_pil = TF.to_pil_image(compressed_image)\n",
    "    generated_image_pil = TF.to_pil_image(generated_image)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize)\n",
    "    ax1.imshow(original_image_pil)\n",
    "    ax1.set_title('high quality')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(generated_image_pil)\n",
    "    ax2.set_title('reconstructed')\n",
    "    ax2.axis('off')\n",
    "    ax3.imshow(compressed_image_pil)\n",
    "    ax3.set_title('low quality')\n",
    "    ax3.axis('off')\n",
    "    fig.subplots_adjust(\n",
    "        top=1.0, bottom=0.0, right=1.0, left=0.0, hspace=0.0, wspace=0.0\n",
    "    )\n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f774474b43a20ce26305101b7f5844f986dc13ec8a91c410b215dc004257e9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
